{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## Exercise 3a: Predicting Risk Levels in Attack Surface Management Data Using Time Series\n",
    "\n",
    "This notebook demonstrates how to use data from your environment to predict future risk levels. If you have risk scores assigned to some vulnerabilities, or to assets in your environment, do you think you can predict what the future might look like based on these scores? And do you know what types of assets contribute most to your risk?\n",
    "\n",
    "**What's the story?**\n",
    "\n",
    "You are still a new analyst in your security operations center, but you now have a better idea of what assets are present in your environment. Your Chief Information Security Officer (CISO) asks you to help her understand the risk and attack surface exposure you carry in your organization. You start to wonder if you can use this data to understand your current risk levels and to predict what the future may hold. This requires some analytics!\n",
    "\n",
    "### Steps we will take:\n",
    "- Load and preprocess the data\n",
    "- Explore key features\n",
    "- Train and evaluate predictive models\n",
    "- Forecast risk for new assets\n",
    "\n",
    "## What model will we use?\n",
    "For this exercise, we recognize that we are working with time series data, or data that is recorded over time intervals (such as a datapoint recorded once each day). We will use an ARIMA time series model to model this data. ARIMA stands for *AutoRegressive Integrated Moving Average*. Itâ€™s a way to understand patterns in time series data and to make predictions about future values based on these patterns. Let's break that down:\n",
    "\n",
    "**AutoRegressive (AR):**\n",
    "\n",
    "This part of the model looks at how the current value of the series relates to its past values.\n",
    "Imagine you are trying to predict tomorrow's temperature based on the temperatures of the past few days. If it was getting warmer each day, you might expect that pattern to continue.\n",
    "\n",
    "**Integrated (I):**\n",
    "\n",
    "This aspect involves differencing the data to make it stationary. Stationary means that the statistical properties of the series like mean and variance are constant over time.\n",
    "For example, if temperatures are increasing every year, you might look at the changes in temperature from day to day (instead of the raw temperatures) to remove that trend so that the data is easier to analyze.\n",
    "\n",
    "**Moving Average (MA):**\n",
    "\n",
    "The moving average part of the model considers the influence of past forecast errors on current values.\n",
    "If you consistently predict too high or too low, this component helps adjust predictions based on those past errors.\n",
    "In an ARIMA model, you are combining these three elements to create a formula that tries to explain the time series data. \n",
    "\n",
    "**Note: all time series models require some assumptions about your data to have been validated in order for the model to truly apply and be appropriate for modeling and predicting for your situation. For more information about these assumptions and about time series models, see a resource such as this:** https://otexts.com/fpp2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Adjust the file path as necessary\n",
    "df = pd.read_csv('../data/attack_surface_management_data_part_3.csv')\n",
    "\n",
    "# let's filter the data to just the vulnerabilities that are unpatched\n",
    "df = df[df['patch_status'] == 'Unpatched']\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Calculate the average risk per day\n",
    "daily_avg_risk = df.groupby(df['timestamp'].dt.date)['risk_score'].mean()\n",
    "\n",
    "# Convert the resulting series to a DataFrame with a proper index\n",
    "daily_avg_risk = daily_avg_risk.asfreq('D')  # Ensure we have the daily frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hold back a week worth of data to see if our model can predict the risk scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_size = int(len(daily_avg_risk) - 10)\n",
    "training_data, test_data = daily_avg_risk[:training_data_size], daily_avg_risk[training_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the time series of average daily risk training data\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(training_data.index, training_data.values, marker='o', linestyle='-')\n",
    "plt.title('Average Daily Risk Score from Vulnerabilities')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Risk Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember what we saw previously. Do you see the same dip in average risk scores on some days in a predictable pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the ARIMA model (ensure your index is datetime with proper frequency)\n",
    "model = ARIMA(training_data, order=(7, 1, 1))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecasting the next 7 days\n",
    "forecast = model_fit.forecast(steps=10)\n",
    "forecast_index = pd.date_range(start=training_data.index[-1] + pd.Timedelta(days=1), \n",
    "                                periods=10, freq='D')  # Ensure the frequency is daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(daily_avg_risk.index, daily_avg_risk.values, marker='o', linestyle='-', label='Historical Risk')\n",
    "plt.plot(forecast.index, forecast, marker='o', linestyle='-', color='red', label='Forecasted Risk')\n",
    "plt.title('Average Daily Risk Score from Vulnerabilities with Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Risk Score')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which asset types are associated with higher risk scores? Is it because we have more of those types of assets, or because they have vulnerabilities that are more critical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze risk by asset type\n",
    "risk_by_asset_type = df.groupby('asset_type')['risk_score'].mean()\n",
    "\n",
    "# Plotting risk by asset type\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=risk_by_asset_type.index, y=risk_by_asset_type.values)\n",
    "plt.title('Average Risk Score by Asset Type')\n",
    "plt.xlabel('Asset Type')\n",
    "plt.ylabel('Average Risk Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did our model do?\n",
    "# Calculate errors\n",
    "mae = mean_absolute_error(test_data, forecast)\n",
    "mse = mean_squared_error(test_data, forecast)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# Lower values represent a better fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Key Takeaways:\n",
    "- **It is possible to predict the future using math models that aren't machine learning or AI models!**\n",
    "- **In this exercise, there was more risk on average due to cloud assets**\n",
    "- **Predictive models can forecast risk for new assets**\n",
    "\n",
    "## What were our steps?\n",
    "**We did some data cleaning, asked ourselves some critical questions about our data, and about what kind of model is appropriate, and fit the model. Once you fit a model, it's important to have some kind of method for figuring out if your predictions were accurate-- models can be updated with new data!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food for Thought:\n",
    "\n",
    "1. Why not GenAI for this? \n",
    "2. What can be changed in this approach?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
